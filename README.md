# PIC-MDK

Module develoment kit for particle-in-cell codes

## Introduction and Motivation

In recent years there has been an active development of software for PIC plasma simulation. Nowadays, there seems to be several well-established PIC codes widely employed by the community. Additionally, there are many somewhat smaller scale, yet competitive and actively used by its developers and collaborators, PIC codes and libraries. With more and more PIC-related software being developed and an ongoing shift towards open source software, community collaboration around widely used data formats, libraries and other tools is beneficial for creating and extending high-quality software, exchanging ideas and developments.

There are some important steps already taken in this direction. The [openPMD standard](http://www.openpmd.org) covers meta data and naming schemes for results of PIC simulations, allowing better compatibility and validation between codes and creating potential for common postprocessing and visualization tools. The [PICSAR project](https://picsar.net/) concerns high-performance implementation of PIC core routines for parallel systems on shared and distributed memory. Both areas are undeniably important: with continuous advancement in computer architectures there are more and more low-level details to take into account for high-performance PIC implementation (data layout, parallelism and load balancing, vectorization, cache efficiency, asynchronous operations, etc.); the volume of results of simulation grows rapidly and sophisticated postprocessing/analysis/visualization tools are needed.

With the mentioned initiatives covering the aspects of high-performance PIC core implementation and output data markup, we feel there is another major area not properly covered yet. In addition to the core operations of the PIC method, many simulations require additional stages of the computational loop to account for additional physical effects (such as QED) or gather data for analysis/output/visualization (for example, track trajectories of certain particles). Many of such extensions and output tools have become fairly standard and thus similar versions of those have been implemented over and over again. Although a few well-established codes are open source and even have some extension/plugin capabilities of their own, porting extensions between them is practically hard due to differences in programming languages, data layout, organization of parallel processing, intricate implementation details, etc. A standardized way of developing extensions compatible with several PIC cores, would greatly simplify creating and sharing such extensions and benefit the community. It is important to provide an abstraction from low-level implementation details such as data layout and organization of parallel processing as well as utilities for commonly needed features (notably, synchronization of data between different extensions, threads or processes), to simplify creating extensions. Another important aspect is that in this case such extensions can be published and distributed as separate pieces of software, compatible with all PIC codes supporting the standard.
We have been working on creating such a tool for extensions compatible with PICADOR and ELMIS codes for about 4 years, some ideas of our prior work in this direction are given in the appendix of [our earlier work](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.92.023305). Based on our experience we are currently developing a new, extended version of that library, which will be open source, is no longer dependent on PICADOR or ELMIS and can be compatible with other PIC codes (or libraries providing kernels for PIC codes) in several programming languages.

*PIC-MDK* (module development kit for PIC codes) is an open source library for developing extensions for PIC codes, which account for additional physical effects, visualization and diagnostics. Such extensions are referred to as modules. The modules are written independently of core data structures and implementation details of PIC codes and rely only on library abstractions. To support *PIC-MDK* a developer of a PIC code creates an adapter between core data structures and its abstractions by implementing wrappers. Modules developed using *PIC-MDK* are portable between all PIC codes, which have adapters.

## Main Concepts

*PIC-MDK* is based on event-handler model. Throughout the PIC simulation there are certain events occurring, such as start of a time iteration, pushing a particle, end of processing of particles and cells on the current time step, and many others. *PIC-MDK* allows creating extensions of the standard PIC loop by writing handlers and assigning them to events. Handlers depend only on *PIC-MDK* abstractions representing particle, grid and other entities of the simulation and thus are agnostic of PIC core data structures and parallelism schemes. *PIC-MDK* provides utilities to simplify creating such extensions and performs dispatch of handlers once an event occurs. A module is a set of handlers working together to accomplish some task.

There are several types of handlers in *PIC-MDK*, reflecting distinct levels of parallelism and workload granularity available in modern PIC codes. On the finest granularity level are operations with single particles or grid values, which are performed independently and usually in parallel using e.g. OpenMP. Then, there are operations involving the full set of particles and/or grid values available to the MPI process (or subarea being processed). Finally, some operations work on the level of the whole simulation and require communication between MPI processes.

*PIC-MDK* provides 5 types of handlers: domain, particle, cell, output and custom handlers. The first 4 are specialized versions of a custom handler fit for widely used cases and easier to use. Domain handler is called at the start of each time step and operates with the whole ensemble (set) of particles and grid via *PIC-MDK* abstractions; there is a unique entity of a domain handler for each MPI process. Particle and cell handlers are called inside loops over particles and cells, respectively; such loops are often done in parallel, so to avoid data races there is a unique entity of such handlers for each thread. Output handlers are called at the end of each time step and are supposed to do some form of output of data collected by other handlers: writing to file, plotting, etc.; there is a unique entity per the whole simulation. Custom handlers can be registered for one or several events and provide the most general, and somewhat harder to use, case. Handlers have access to the current data being processed (depending on the handler type) as well as general parameters of simulation (box size, time step, etc.).

There are also dynamic events, which do not have a fixed place in the PIC loop and depend on the simulation results. Examples are changing simulation parameters (e.g. time step change or load balancing between MPI processes), a particle leaving simulation area, an electron-positron pair being generated, etc.

For many extensions, particularly those concerning data gathering and analysis, it if often needed to exchange and synchronize data between several extensions, threads or MPI processes. For example, computing energy of all electrons could be done by processing each electron in a loop over particles and, later, adding up the results for each thread and process. Similar considerations can be applied to data for spectra, particle density and other types of output. It is generally beneficial to have data gathering and data exchange/synchronization separately for both code organization and performance reasons. Therefore, it is typical that some handlers of a module only gather the data and others process it later.

*PIC-MDK* provides objects to represent datasets as single values, 1D, 2D and 3D arrays of various data types. Handlers can interact with each other by registering and retrieving datasets by text names using the provided InterData object, available to each handler. Registering a dataset includes specifying its text name and, optionally, synchronization mode: event, operation (reduction operations, gather), whether to perform it on shared or distributed memory levels (the latter is done by the corresponding MPI operations). All registered datasets are synchronized automatically once the specified event occurs.

For example, the mentioned above module computing energy of electrons can be organized as follows. It consists of two handlers. One is a particle handler which registers a dataset of a single real value with global sum synchronization after the particles loop. Another handler is for output, it takes the synchronized value (the matching is done by text names) and, for example, prints it to a file. Let us briefly describe how does this scheme work. All operations concerning registering data are done in a specialized place so that after the initialization phase the InterData has full information and is capable of matching all ‘import’ and ‘export’ requests. For each MPI process and OpenMP thread used there is a unique entity of the particle handler and the corresponding registered value. During the parallel loop over particles the handler accumulates the energy of processed particles to its own value, so there is no need for synchronization. When the post particle loop event occurs, these values are synchronized first between threads and then between MPI processes to get the final value, corresponding to energy in the whole simulation area. Due to the matching done by InterData, this value will be placed to the dataset used by the output handler. Thus, from this handler’s point of view, the synchronized data will automatically appear in the corresponding dataset.

## Using PIC-MDK

*PIC-MDK* is currently in the alpha version. We plan to release a beta version on Autumn 2017.

*PIC-MDK* is intended to be compatible with different PIC codes and modules written in different programming languages. Currently, the developed alpha version provides C and C++ interfaces for writing modules. It also includes various utility features to simplify this process, such as tools to read config files, report errors, save datasets to files (openPMD-compatible formats in plan), wrappers for MPI and OpenMP.

Modules using the C++ interface follow the object-oriented approach with inheriting base classes of the library and implementing virtual functions. Modules using the C interface follow procedural approach with each handler being a function with a given interface and taking logical equivalents of C++ objects passed via opaque pointers. In this way, one could write handlers and modules not only in C, but in other languages capable of calling C functions, such as Fortran.

Modifying an existing code to support *PIC-MDK* consists of creating an adapter between internal data structures, such as particles and grid, and *PIC-MDK* abstractions. For C++ codes *PIC-MDK* defines interfaces to be implemented by adapters, given in the Appendix. By means of template instantiation data types from adapters are substituted to modules, thus potentially providing a zero-overhead abstraction.

## Licence

*PIC-MDK* is licensed under the MIT licence. For a detailed description, please refer to [LICENSE](../master/LICENSE).
